{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "sol.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daishwary/ImageClassifier/blob/master/sol.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWkDHJQHNlzF"
      },
      "source": [
        "\n",
        "\n",
        "A model to predict the value of the question in the TV game show  “Jeopardy!”.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "yp1Wd0R2NlzK"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GlobalMaxPooling1D, LSTM, Bidirectional, Embedding, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hd9YIE9NlzL"
      },
      "source": [
        "# 1. Preprocessing\n",
        "\n",
        "The first step is to load the data (in CSV format), and split the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "fMcd7SdyNlzM",
        "outputId": "754ef56f-eca3-4d13-aa7a-98c61135fdc5"
      },
      "source": [
        "data_df = pd.read_csv('/content/JEOPARDY_CSV.csv')\n",
        "data_df = data_df[data_df[' Value'] != 'None']\n",
        "\n",
        "print(data_df.shape)\n",
        "data_df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(213296, 7)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Show Number</th>\n",
              "      <th>Air Date</th>\n",
              "      <th>Round</th>\n",
              "      <th>Category</th>\n",
              "      <th>Value</th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4680</td>\n",
              "      <td>2004-12-31</td>\n",
              "      <td>Jeopardy!</td>\n",
              "      <td>HISTORY</td>\n",
              "      <td>$200</td>\n",
              "      <td>For the last 8 years of his life, Galileo was ...</td>\n",
              "      <td>Copernicus</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4680</td>\n",
              "      <td>2004-12-31</td>\n",
              "      <td>Jeopardy!</td>\n",
              "      <td>ESPN's TOP 10 ALL-TIME ATHLETES</td>\n",
              "      <td>$200</td>\n",
              "      <td>No. 2: 1912 Olympian; football star at Carlisl...</td>\n",
              "      <td>Jim Thorpe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4680</td>\n",
              "      <td>2004-12-31</td>\n",
              "      <td>Jeopardy!</td>\n",
              "      <td>EVERYBODY TALKS ABOUT IT...</td>\n",
              "      <td>$200</td>\n",
              "      <td>The city of Yuma in this state has a record av...</td>\n",
              "      <td>Arizona</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4680</td>\n",
              "      <td>2004-12-31</td>\n",
              "      <td>Jeopardy!</td>\n",
              "      <td>THE COMPANY LINE</td>\n",
              "      <td>$200</td>\n",
              "      <td>In 1963, live on \"The Art Linkletter Show\", th...</td>\n",
              "      <td>McDonald's</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4680</td>\n",
              "      <td>2004-12-31</td>\n",
              "      <td>Jeopardy!</td>\n",
              "      <td>EPITAPHS &amp; TRIBUTES</td>\n",
              "      <td>$200</td>\n",
              "      <td>Signer of the Dec. of Indep., framer of the Co...</td>\n",
              "      <td>John Adams</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Show Number  ...      Answer\n",
              "0         4680  ...  Copernicus\n",
              "1         4680  ...  Jim Thorpe\n",
              "2         4680  ...     Arizona\n",
              "3         4680  ...  McDonald's\n",
              "4         4680  ...  John Adams\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee9MptVdNlzN"
      },
      "source": [
        "## Creating bins\n",
        "\n",
        "Since the values could easily vary, this means we would have way too many classes to classify! Instead, we will bin it in this way: if the value is smaller than 1000, then we round to the nearest hundred. Otherwise, if it's between 1000 and 10k, we round it to nearest thousand. If it's greater than 10k, then we round it to the nearest 10-thousand."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZYr80UgNlzN"
      },
      "source": [
        "data_df['ValueNum'] = data_df[' Value'].apply(\n",
        "    lambda value: int(value.replace(',', '').replace('$', ''))\n",
        ")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBffyuVgNlzN"
      },
      "source": [
        "def binning(value):\n",
        "    if value < 1000:\n",
        "        return np.round(value, -2)\n",
        "    elif value < 10000:\n",
        "        return np.round(value, -3)\n",
        "    else:\n",
        "        return np.round(value, -4)\n",
        "\n",
        "data_df['ValueBins'] = data_df['ValueNum'].apply(binning)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xTRIO-nNlzO",
        "outputId": "b7fa5833-f21a-43f2-943f-482d0fd3f336"
      },
      "source": [
        "print(\"Total number of categories:\", data_df[' Value'].unique().shape[0])\n",
        "print(\"Number of categories after binning:\", data_df['ValueBins'].unique().shape[0])\n",
        "print(\"\\nBinned Categories:\", data_df['ValueBins'].unique())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of categories: 149\n",
            "Number of categories after binning: 21\n",
            "\n",
            "Binned Categories: [  200   400   600   800  2000  1000  3000  5000   100   300   500  4000\n",
            "  7000   700  8000  6000 10000   900  9000     0 20000]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhyNmCgFNlzO"
      },
      "source": [
        "Then, we will split our data by randomly selected 20% of the shows, and use the questions from that show as what we will try to predict."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikpBew7DNlzO"
      },
      "source": [
        "show_numbers = data_df['Show Number'].unique()\n",
        "train_shows, test_shows = train_test_split(show_numbers, test_size=0.2, random_state=2019)\n",
        "\n",
        "train_mask = data_df['Show Number'].isin(train_shows)\n",
        "test_mask = data_df['Show Number'].isin(test_shows)\n",
        "\n",
        "train_labels = data_df.loc[train_mask, 'ValueBins']\n",
        "train_questions = data_df.loc[train_mask, ' Question']\n",
        "test_labels = data_df.loc[test_mask, 'ValueBins']\n",
        "test_questions = data_df.loc[test_mask, ' Question']"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "collapsed": true,
        "id": "2Xuk246vNlzP"
      },
      "source": [
        "# 2. Simple Linear Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgWmviRINlzP"
      },
      "source": [
        "## Transform questions to bag-of-words\n",
        "\n",
        "Bag of words is a very simple, but very convenient way of representing any type of freeform text using vectors.\n",
        "\n",
        "In our model, we will limit ourselves to only using the top 2000 most frequent words as features, in order for the logistic regression model to not overfit on too many features. Further, we are removing **stop words**, which are very common words in English that we wish to remove in order to only keep relevant information.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8l_4HTpNlzP",
        "outputId": "6ff933e5-5307-4fc3-ee4f-7c4daa8b69a4"
      },
      "source": [
        "%%time\n",
        "bow = CountVectorizer(stop_words='english', max_features=2000)\n",
        "bow.fit(data_df[' Question'])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3.15 s, sys: 29.7 ms, total: 3.18 s\n",
            "Wall time: 3.19 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oI1JT80NlzQ",
        "outputId": "e30f4559-e0b0-4b22-9132-0fa239863900"
      },
      "source": [
        "X_train = bow.transform(train_questions)\n",
        "X_test = bow.transform(test_questions)\n",
        "\n",
        "y_train = train_labels\n",
        "y_test = test_labels\n",
        "\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of X_train: (170704, 2000)\n",
            "Shape of X_test: (42592, 2000)\n",
            "Shape of y_train: (170704,)\n",
            "Shape of y_test: (42592,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPpmVLFMNlzQ"
      },
      "source": [
        "## Train the Logistic Regression model\n",
        "\n",
        "Logistic Regression is perhaps the simplest regression model out there."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuEYEmqFNlzQ",
        "outputId": "e9e50ef1-0dd5-4341-8304-3a3e1f989049"
      },
      "source": [
        "%%time\n",
        "lr = LogisticRegression(solver='saga', multi_class='multinomial', max_iter=200)\n",
        "lr.fit(X_train, y_train)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 10s, sys: 22.5 ms, total: 1min 10s\n",
            "Wall time: 1min 10s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n86ectywNlzQ"
      },
      "source": [
        "## Evaluate the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hILiSwQ1NlzR",
        "outputId": "fd5f9e81-b0cb-4d05-8d06-ee026243b044"
      },
      "source": [
        "y_pred = lr.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         2\n",
            "         100       0.05      0.00      0.01      1863\n",
            "         200       0.18      0.14      0.15      6132\n",
            "         300       0.06      0.00      0.01      1801\n",
            "         400       0.21      0.57      0.30      8425\n",
            "         500       0.10      0.01      0.02      1827\n",
            "         600       0.11      0.01      0.02      4099\n",
            "         700       0.00      0.00      0.00        41\n",
            "         800       0.15      0.10      0.12      6279\n",
            "         900       0.00      0.00      0.00        28\n",
            "        1000       0.19      0.20      0.20      6720\n",
            "        2000       0.19      0.10      0.13      4938\n",
            "        3000       0.00      0.00      0.00       198\n",
            "        4000       0.00      0.00      0.00       121\n",
            "        5000       0.00      0.00      0.00        61\n",
            "        6000       0.00      0.00      0.00        21\n",
            "        7000       0.00      0.00      0.00         9\n",
            "        8000       0.00      0.00      0.00        11\n",
            "        9000       0.00      0.00      0.00         4\n",
            "       10000       0.00      0.00      0.00        12\n",
            "\n",
            "    accuracy                           0.19     42592\n",
            "   macro avg       0.06      0.06      0.05     42592\n",
            "weighted avg       0.16      0.19      0.15     42592\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "it4r6II1NlzR"
      },
      "source": [
        "# 2. LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfyRGgAHNlzR"
      },
      "source": [
        "## Tokenize & Pad\n",
        "\n",
        "We are doing 3 things here:\n",
        "\n",
        "1. Train a tokenizer in all the text. This tokenizer will create an dictionary mapping words to an index, aka `tokenizer.word_index`.\n",
        "2. Convert the questions (which are strings of text) into a list of list of integers, each representing the index of a word in the `word_index`.\n",
        "3. Pad each \"list of list\" into a single numpy array. To do this, we use the `pad_sequences` function, and set a maximum length (50 is reasonable since most questions will be at most 20 words), after which any word is cutoff.\n",
        "\n",
        "Note:\n",
        "* Tokenizer will take at most 50k words. Here, we are using more words than Logistic Regression since the input dimension does not account for **all** words, but only the words that are actually given in the sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZtQBICzNlzR",
        "outputId": "6ea29511-bafb-4fe0-9f3b-9461844e68ae"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=50000)\n",
        "tokenizer.fit_on_texts(data_df[' Question'])\n",
        "\n",
        "train_sequence = tokenizer.texts_to_sequences(train_questions)\n",
        "test_sequence = tokenizer.texts_to_sequences(test_questions)\n",
        "\n",
        "print(\"Original text:\", train_questions[0])\n",
        "print(\"Converted sequence:\", train_sequence[0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original text: For the last 8 years of his life, Galileo was under house arrest for espousing this man's theory\n",
            "Converted sequence: [7, 1, 112, 272, 102, 4, 14, 189, 7842, 9, 226, 173, 5422, 7, 41554, 2, 571, 1552]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ierxZnzVNlzS",
        "outputId": "422856e0-cf29-4615-c9ff-e7c0a37fe6b8"
      },
      "source": [
        "X_train = pad_sequences(train_sequence, maxlen=50)\n",
        "X_test = pad_sequences(test_sequence, maxlen=50)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(170704, 50)\n",
            "(42592, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLPjCfI5NlzS"
      },
      "source": [
        "## Encode labels as counts\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaauasdGNlzS",
        "outputId": "c78d1e93-79ee-4bd2-e734-80c860caafda"
      },
      "source": [
        "le = LabelEncoder()\n",
        "le.fit(data_df['ValueBins'])\n",
        "\n",
        "y_train = le.transform(train_labels)\n",
        "y_test = le.transform(test_labels)\n",
        "\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(170704,)\n",
            "(42592,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZLcRoM_NlzS"
      },
      "source": [
        "## Building and running the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzu1jX7yNlzT"
      },
      "source": [
        "num_words = tokenizer.num_words\n",
        "output_size = len(le.classes_)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nZnT17GNlzT",
        "outputId": "5e2f433d-0691-4738-d82f-a08b5bf2932d"
      },
      "source": [
        "model = Sequential([\n",
        "    Embedding(input_dim=num_words, \n",
        "              output_dim=200, \n",
        "              mask_zero=True, \n",
        "              input_length=50),\n",
        "    Bidirectional(LSTM(150, return_sequences=True)),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(300, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(output_size, activation='softmax')\n",
        "    \n",
        "])\n",
        "\n",
        "model.compile('adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 50, 200)           10000000  \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 50, 300)           421200    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 300)               90300     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 21)                6321      \n",
            "=================================================================\n",
            "Total params: 10,517,821\n",
            "Trainable params: 10,517,821\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzWPlTtXNlzT"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DD69xWycNlzT",
        "outputId": "b3f72122-12a6-4f3c-a55d-514b8d0855ae"
      },
      "source": [
        "model.fit(X_train, y_train, epochs=10, batch_size=1024, validation_split=0.1)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "151/151 [==============================] - 542s 4s/step - loss: 2.4098 - accuracy: 0.1676 - val_loss: 2.1073 - val_accuracy: 0.2068\n",
            "Epoch 2/10\n",
            "151/151 [==============================] - 525s 3s/step - loss: 2.0927 - accuracy: 0.2091 - val_loss: 2.0927 - val_accuracy: 0.2161\n",
            "Epoch 3/10\n",
            "151/151 [==============================] - 542s 4s/step - loss: 1.9770 - accuracy: 0.2531 - val_loss: 2.1355 - val_accuracy: 0.1972\n",
            "Epoch 4/10\n",
            "151/151 [==============================] - 537s 4s/step - loss: 1.7786 - accuracy: 0.3455 - val_loss: 2.2707 - val_accuracy: 0.1824\n",
            "Epoch 5/10\n",
            "151/151 [==============================] - 541s 4s/step - loss: 1.5133 - accuracy: 0.4640 - val_loss: 2.5372 - val_accuracy: 0.1799\n",
            "Epoch 6/10\n",
            "151/151 [==============================] - 542s 4s/step - loss: 1.2424 - accuracy: 0.5670 - val_loss: 3.0529 - val_accuracy: 0.1726\n",
            "Epoch 7/10\n",
            "151/151 [==============================] - 550s 4s/step - loss: 1.0186 - accuracy: 0.6466 - val_loss: 3.5554 - val_accuracy: 0.1727\n",
            "Epoch 8/10\n",
            "151/151 [==============================] - 546s 4s/step - loss: 0.8328 - accuracy: 0.7119 - val_loss: 4.1752 - val_accuracy: 0.1672\n",
            "Epoch 9/10\n",
            "151/151 [==============================] - 558s 4s/step - loss: 0.6913 - accuracy: 0.7638 - val_loss: 4.8788 - val_accuracy: 0.1645\n",
            "Epoch 10/10\n",
            "151/151 [==============================] - 550s 4s/step - loss: 0.5775 - accuracy: 0.8022 - val_loss: 5.5990 - val_accuracy: 0.1642\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff9ef095410>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JostvWfNlzT"
      },
      "source": [
        "## Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvgJz_RGNlzU",
        "outputId": "36d57e3a-235a-4ee2-963b-a5b4f8f91265"
      },
      "source": [
        "y_pred = model.predict(X_test, batch_size=1024).argmax(axis=1)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         2\n",
            "           1       0.07      0.05      0.06      1863\n",
            "           2       0.18      0.20      0.19      6132\n",
            "           3       0.06      0.04      0.05      1801\n",
            "           4       0.21      0.19      0.20      8425\n",
            "           5       0.07      0.06      0.06      1827\n",
            "           6       0.11      0.10      0.10      4099\n",
            "           7       0.00      0.00      0.00        41\n",
            "           8       0.16      0.17      0.16      6279\n",
            "           9       0.00      0.00      0.00        28\n",
            "          10       0.19      0.21      0.20      6720\n",
            "          11       0.17      0.18      0.17      4938\n",
            "          12       0.01      0.01      0.01       198\n",
            "          13       0.00      0.00      0.00       121\n",
            "          14       0.00      0.00      0.00        61\n",
            "          15       0.00      0.00      0.00        21\n",
            "          16       0.00      0.00      0.00         9\n",
            "          17       0.00      0.00      0.00        11\n",
            "          18       0.00      0.00      0.00         4\n",
            "          19       0.00      0.00      0.00        12\n",
            "\n",
            "    accuracy                           0.16     42592\n",
            "   macro avg       0.06      0.06      0.06     42592\n",
            "weighted avg       0.16      0.16      0.16     42592\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}